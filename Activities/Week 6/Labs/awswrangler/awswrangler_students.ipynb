{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5a7b8b",
   "metadata": {},
   "source": [
    "# Introduction to AWS SDK for Pandas (awswrangler)\n",
    "\n",
    "In this session, we'll explore how to use the **AWS SDK for Pandas**, also known as `awswrangler`, to simplify interactions with AWS services like S3. This library provides a higher-level interface compared to Boto3, making it easier to perform data engineering tasks directly in Python.\n",
    "\n",
    "We'll start with a brief refresher on using Boto3 to interact with S3 and then move on to using `awswrangler` to achieve the same tasks more efficiently.\n",
    "\n",
    "**If you get stuck, there is no better place than reading the documentation which you can find here: https://aws-sdk-pandas.readthedocs.io/en/stable/**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b54a22",
   "metadata": {},
   "source": [
    "## Installaion and Setup\n",
    "\n",
    "To install awswrangler you can install using either `uv pip install`, `pip install` or `conda install`\n",
    "\n",
    "```shell\n",
    "uv pip install awswrangler\n",
    "```\n",
    "or\n",
    "```shell\n",
    "pip install awswrangler\n",
    "```\n",
    "\n",
    "Once installed, make sure you have your `.env` file created with the following information\n",
    "```\n",
    "AWS_ACCESS_KEY_ID=<YOURKEY>\n",
    "AWS_SECRET_ACCESS_KEY=<YOURSECRETKEY>\n",
    "AWS_DEFAULT_REGION=us-east-2\n",
    "```\n",
    "\n",
    "If you did install python-dotenv before then you can do so using:\n",
    "```shell\n",
    "uv pip install python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbaf667a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load your credentials\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7c97be",
   "metadata": {},
   "source": [
    "## Getting to know awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e261e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import awswrangler\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347eb66",
   "metadata": {},
   "source": [
    "Similar to pandas, awswrangles comes with many reader and writer functions. You can use `wr.s3.read_csv`, `wr.s3.read_parquet`, ..etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9186c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/2/2025 16:00:00</td>\n",
       "      <td>191.49</td>\n",
       "      <td>193.20</td>\n",
       "      <td>188.71</td>\n",
       "      <td>190.63</td>\n",
       "      <td>17545162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2025 16:00:00</td>\n",
       "      <td>192.73</td>\n",
       "      <td>194.50</td>\n",
       "      <td>191.35</td>\n",
       "      <td>193.13</td>\n",
       "      <td>12874957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/6/2025 16:00:00</td>\n",
       "      <td>195.15</td>\n",
       "      <td>199.56</td>\n",
       "      <td>195.06</td>\n",
       "      <td>197.96</td>\n",
       "      <td>19483323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2025 16:00:00</td>\n",
       "      <td>198.27</td>\n",
       "      <td>202.14</td>\n",
       "      <td>195.94</td>\n",
       "      <td>196.71</td>\n",
       "      <td>16966760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2025 16:00:00</td>\n",
       "      <td>193.95</td>\n",
       "      <td>197.64</td>\n",
       "      <td>193.75</td>\n",
       "      <td>195.39</td>\n",
       "      <td>14335341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Date    Open    High     Low   Close    Volume\n",
       "0  1/2/2025 16:00:00  191.49  193.20  188.71  190.63  17545162\n",
       "1  1/3/2025 16:00:00  192.73  194.50  191.35  193.13  12874957\n",
       "2  1/6/2025 16:00:00  195.15  199.56  195.06  197.96  19483323\n",
       "3  1/7/2025 16:00:00  198.27  202.14  195.94  196.71  16966760\n",
       "4  1/8/2025 16:00:00  193.95  197.64  193.75  195.39  14335341"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if you can read from a private bucket\n",
    "\n",
    "df = wr.s3.read_csv('s3://techcatalyst-raw/stocks/GOOG.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d723939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140 entries, 0 to 139\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    140 non-null    object \n",
      " 1   Open    140 non-null    float64\n",
      " 2   High    140 non-null    float64\n",
      " 3   Low     140 non-null    float64\n",
      " 4   Close   140 non-null    float64\n",
      " 5   Volume  140 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 6.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# the returned object is actuall a pandas DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd00585",
   "metadata": {},
   "source": [
    "Now, you have a pandas DataFrame loaded into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a2360",
   "metadata": {},
   "source": [
    "In AWS Glue, you worked with Glue Catalog, Glue Database, and Glue Tables. If you recall, you used the Glue Crawler to crawl your files in an S3 bucket, then the crawler would populate the metadata information in a Glue Table inside the Glue Database you had to define manually. \n",
    "\n",
    "Let's inspect what Glue Database we have available using `awswrangler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f18f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Database Description\n",
      "0  awswrangler_test            \n",
      "1             my_db            \n"
     ]
    }
   ],
   "source": [
    "databases = wr.catalog.databases()\n",
    "print(databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9065a9",
   "metadata": {},
   "source": [
    "These are databases I had created. You will need to create you own database now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = # ENTER YOUR NAME HERE\n",
    "database_name = f\"{name}_db\"\n",
    "wr.catalog.create_database(database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d319d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awswrangler_test</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my_db</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tatwan_db</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Database Description\n",
       "0  awswrangler_test            \n",
       "1             my_db            \n",
       "2         tatwan_db            "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm it was created \n",
    "wr.catalog.databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42001cf3",
   "metadata": {},
   "source": [
    "Inspect your database to see if it contains any tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db188144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Database, Table, Description, TableType, Columns, Partitions]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(database=database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c69bb5",
   "metadata": {},
   "source": [
    "awswrangles allows you to easily write you pandas DataFrame into S3 in any format (e.g., CSV, Parquet, JSON ..etc). You can also specify a Glue Database and a Table so it can write some metadata information. Let's see how that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b754dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://techcatalyst-raw/tatwan/36385087362744ee918c24a23b82e878.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=f\"s3://techcatalyst-raw/{name}/\", # write to the techcatalyst-raw bucket under your folder name (or it would create a new folder)\n",
    "    dataset=True, \n",
    "    database=database_name, # the name of the database you just created in AWS Glue \n",
    "    table= #YOUR CODE, # pick a table name for example YOURNAME_STOCK\n",
    "    mode='overwrite'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe3740",
   "metadata": {},
   "source": [
    "The above line confirmst that the parquet file was written in the specified S3 bucket. Let's inspect if a table was written in AWS Glue. You should login to the AWS console (Web), change region to use-east-2, and then navigate to AWS Glue. Check the Database and verify it was created. Then inside that database, verify that the table was created. \n",
    "\n",
    "We can also do this via awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aee8622e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tatwan_db</td>\n",
       "      <td>tatwan_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Database         Table Description       TableType  \\\n",
       "0  tatwan_db  tatwan_stock              EXTERNAL_TABLE   \n",
       "\n",
       "                                Columns Partitions  \n",
       "0  date, open, high, low, close, volume             "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(database=database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "254e118e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tatwan_db</td>\n",
       "      <td>tatwan_stock</td>\n",
       "      <td></td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>date, open, high, low, close, volume</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Database         Table Description       TableType  \\\n",
       "0  tatwan_db  tatwan_stock              EXTERNAL_TABLE   \n",
       "\n",
       "                                Columns Partitions  \n",
       "0  date, open, high, low, close, volume             "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also do a search for tables by name\n",
    "wr.catalog.tables(name_contains=\"stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ecc91d",
   "metadata": {},
   "source": [
    "To view the content of a specific AWS Glue table using awswrangler, you typically read the data from the underlying data store (often S3) using the Glue table’s catalog metadata. This is how Athena works by utilizing Glue tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc163fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/2/2025 16:00:00</td>\n",
       "      <td>191.49</td>\n",
       "      <td>193.20</td>\n",
       "      <td>188.71</td>\n",
       "      <td>190.63</td>\n",
       "      <td>17545162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/3/2025 16:00:00</td>\n",
       "      <td>192.73</td>\n",
       "      <td>194.50</td>\n",
       "      <td>191.35</td>\n",
       "      <td>193.13</td>\n",
       "      <td>12874957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/6/2025 16:00:00</td>\n",
       "      <td>195.15</td>\n",
       "      <td>199.56</td>\n",
       "      <td>195.06</td>\n",
       "      <td>197.96</td>\n",
       "      <td>19483323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/7/2025 16:00:00</td>\n",
       "      <td>198.27</td>\n",
       "      <td>202.14</td>\n",
       "      <td>195.94</td>\n",
       "      <td>196.71</td>\n",
       "      <td>16966760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/8/2025 16:00:00</td>\n",
       "      <td>193.95</td>\n",
       "      <td>197.64</td>\n",
       "      <td>193.75</td>\n",
       "      <td>195.39</td>\n",
       "      <td>14335341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date    open    high     low   close    volume\n",
       "0  1/2/2025 16:00:00  191.49  193.20  188.71  190.63  17545162\n",
       "1  1/3/2025 16:00:00  192.73  194.50  191.35  193.13  12874957\n",
       "2  1/6/2025 16:00:00  195.15  199.56  195.06  197.96  19483323\n",
       "3  1/7/2025 16:00:00  198.27  202.14  195.94  196.71  16966760\n",
       "4  1/8/2025 16:00:00  193.95  197.64  193.75  195.39  14335341"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wr.s3.read_parquet_table(database=database_name, table='TATWAN_STOCK')\n",
    "\n",
    "# Display the DataFrame's first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48eec19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': 'string',\n",
       " 'open': 'double',\n",
       " 'high': 'double',\n",
       " 'low': 'double',\n",
       " 'close': 'double',\n",
       " 'volume': 'bigint'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can just request the column types in the table using the get_table_types\n",
    "wr.catalog.get_table_types(database=database_name, table='TATWAN_STOCK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd26b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Partition</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date</td>\n",
       "      <td>string</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>open</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>close</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>volume</td>\n",
       "      <td>bigint</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Column Name    Type  Partition Comment\n",
       "0        date  string      False        \n",
       "1        open  double      False        \n",
       "2        high  double      False        \n",
       "3         low  double      False        \n",
       "4       close  double      False        \n",
       "5      volume  bigint      False        "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can obtain the same information such as column type, partition, and any comments using the table method \n",
    "wr.catalog.table(database=database_name, table='TATWAN_STOCK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f202ab2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'tatwan_stock',\n",
       " 'DatabaseName': 'tatwan_db',\n",
       " 'Description': 'This is my stock table.',\n",
       " 'CreateTime': datetime.datetime(2025, 7, 31, 14, 15, 56, tzinfo=tzlocal()),\n",
       " 'UpdateTime': datetime.datetime(2025, 7, 31, 14, 31, 21, tzinfo=tzlocal()),\n",
       " 'Retention': 0,\n",
       " 'StorageDescriptor': {'Columns': [{'Name': 'date',\n",
       "    'Type': 'string',\n",
       "    'Comment': 'Trading Date'},\n",
       "   {'Name': 'open', 'Type': 'double', 'Comment': 'Opening Price'},\n",
       "   {'Name': 'high', 'Type': 'double'},\n",
       "   {'Name': 'low', 'Type': 'double'},\n",
       "   {'Name': 'close', 'Type': 'double', 'Comment': 'Closing Price'},\n",
       "   {'Name': 'volume', 'Type': 'bigint'}],\n",
       "  'Location': 's3://techcatalyst-raw/tatwan/',\n",
       "  'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
       "  'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
       "  'Compressed': True,\n",
       "  'NumberOfBuckets': -1,\n",
       "  'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
       "   'Parameters': {'serialization.format': '1'}},\n",
       "  'BucketColumns': [],\n",
       "  'SortColumns': [],\n",
       "  'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "   'compressionType': 'snappy',\n",
       "   'classification': 'parquet',\n",
       "   'typeOfData': 'file'},\n",
       "  'StoredAsSubDirectories': False},\n",
       " 'PartitionKeys': [],\n",
       " 'TableType': 'EXTERNAL_TABLE',\n",
       " 'Parameters': {'compressionType': 'snappy',\n",
       "  'source': 'Google',\n",
       "  'classification': 'parquet',\n",
       "  'projection.enabled': 'false',\n",
       "  'class': 'stock',\n",
       "  'typeOfData': 'file'},\n",
       " 'CreatedBy': 'arn:aws:iam::535146832369:user/tatwan',\n",
       " 'IsRegisteredWithLakeFormation': False,\n",
       " 'CatalogId': '535146832369',\n",
       " 'VersionId': '1',\n",
       " 'IsMultiDialectView': False}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Glue metadata as a generator that contains dictionaries for all tables in your database\n",
    "table_details = wr.catalog.get_tables(database=database_name)\n",
    "\n",
    "next(table_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208d1df",
   "metadata": {},
   "source": [
    "Glue Tables allows us to add additonal metadata information like description about the data, comments about each column ..etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaac047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://techcatalyst-raw/tatwan/8cc73735d3b24af5b1ddfbaf692cf595.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example adding additional metadata information \n",
    "\n",
    "desc = \"This is my stock table.\"\n",
    "\n",
    "param = {\"source\": \"Google\", \"class\": \"stock\"}\n",
    "\n",
    "comments = {\n",
    "    \"Date\": \"Trading Date\",\n",
    "    \"Open\": \"Opening Price\",\n",
    "    \"Close\": \"Closing Price\"\n",
    "}\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=\"s3://techcatalyst-raw/tatwan/\", # CHANGE THIS TO USE YOUR NAME \n",
    "    dataset=True,\n",
    "    database=database_name,\n",
    "    table=#YOURTABLE NAME,\n",
    "    mode='overwrite',\n",
    "    glue_table_settings=wr.typing.GlueTableSettings(description=desc, parameters=param, columns_comments=comments),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce631d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Partition</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date</td>\n",
       "      <td>string</td>\n",
       "      <td>False</td>\n",
       "      <td>Trading Date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>open</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td>Opening Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>close</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td>Closing Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>volume</td>\n",
       "      <td>bigint</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Column Name    Type  Partition        Comment\n",
       "0        date  string      False   Trading Date\n",
       "1        open  double      False  Opening Price\n",
       "2        high  double      False               \n",
       "3         low  double      False               \n",
       "4       close  double      False  Closing Price\n",
       "5      volume  bigint      False               "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can obtain the same information such as column type, partition, and any comments using the table method \n",
    "wr.catalog.table(database=database_name, table=#YOURNAME_STOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4acd6bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_tables at 0x790ac6b8c8e0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can get all tables in all databases\n",
    "tables = wr.catalog.get_tables()\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3be815",
   "metadata": {},
   "source": [
    "You will need to loop through the `generator` to extract specific information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d59c60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_tables at 0x790ac6b8f400>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = wr.catalog.get_tables()\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36c37bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'my_table',\n",
       "  'DatabaseName': 'my_db',\n",
       "  'Description': 'This is my stock table.',\n",
       "  'CreateTime': datetime.datetime(2025, 7, 31, 8, 41, 35, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2025, 7, 31, 8, 45, 16, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'date',\n",
       "     'Type': 'string',\n",
       "     'Comment': 'Trading Date'},\n",
       "    {'Name': 'open', 'Type': 'double', 'Comment': 'Opening Price'},\n",
       "    {'Name': 'high', 'Type': 'double'},\n",
       "    {'Name': 'low', 'Type': 'double'},\n",
       "    {'Name': 'close', 'Type': 'double', 'Comment': 'Closing Price'},\n",
       "    {'Name': 'volume', 'Type': 'bigint'}],\n",
       "   'Location': 's3://techcatalyst-raw/tatwan/',\n",
       "   'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
       "   'Compressed': True,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
       "    'Parameters': {'serialization.format': '1'}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'compressionType': 'snappy',\n",
       "    'classification': 'parquet',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'compressionType': 'snappy',\n",
       "   'source': 'Google',\n",
       "   'classification': 'parquet',\n",
       "   'projection.enabled': 'false',\n",
       "   'class': 'stock',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:iam::535146832369:user/tatwan',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '535146832369',\n",
       "  'VersionId': '1',\n",
       "  'IsMultiDialectView': False},\n",
       " {'Name': 'test',\n",
       "  'DatabaseName': 'my_db',\n",
       "  'CreateTime': datetime.datetime(2025, 7, 31, 9, 12, 51, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2025, 7, 31, 9, 12, 51, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'date', 'Type': 'string'},\n",
       "    {'Name': 'open', 'Type': 'double'},\n",
       "    {'Name': 'high', 'Type': 'double'},\n",
       "    {'Name': 'low', 'Type': 'double'},\n",
       "    {'Name': 'close', 'Type': 'double'},\n",
       "    {'Name': 'volume', 'Type': 'bigint'}],\n",
       "   'Location': 's3://techcatalyst-raw/tatwan/raw/',\n",
       "   'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
       "   'Compressed': True,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
       "    'Parameters': {'serialization.format': '1'}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'compressionType': 'snappy',\n",
       "    'classification': 'parquet',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'compressionType': 'snappy',\n",
       "   'classification': 'parquet',\n",
       "   'projection.enabled': 'false',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:iam::535146832369:user/tatwan',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '535146832369',\n",
       "  'VersionId': '0',\n",
       "  'IsMultiDialectView': False},\n",
       " {'Name': 'tatwan_stock',\n",
       "  'DatabaseName': 'tatwan_db',\n",
       "  'Description': 'This is my stock table.',\n",
       "  'CreateTime': datetime.datetime(2025, 7, 31, 14, 15, 56, tzinfo=tzlocal()),\n",
       "  'UpdateTime': datetime.datetime(2025, 7, 31, 14, 31, 21, tzinfo=tzlocal()),\n",
       "  'Retention': 0,\n",
       "  'StorageDescriptor': {'Columns': [{'Name': 'date',\n",
       "     'Type': 'string',\n",
       "     'Comment': 'Trading Date'},\n",
       "    {'Name': 'open', 'Type': 'double', 'Comment': 'Opening Price'},\n",
       "    {'Name': 'high', 'Type': 'double'},\n",
       "    {'Name': 'low', 'Type': 'double'},\n",
       "    {'Name': 'close', 'Type': 'double', 'Comment': 'Closing Price'},\n",
       "    {'Name': 'volume', 'Type': 'bigint'}],\n",
       "   'Location': 's3://techcatalyst-raw/tatwan/',\n",
       "   'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n",
       "   'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n",
       "   'Compressed': True,\n",
       "   'NumberOfBuckets': -1,\n",
       "   'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n",
       "    'Parameters': {'serialization.format': '1'}},\n",
       "   'BucketColumns': [],\n",
       "   'SortColumns': [],\n",
       "   'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0',\n",
       "    'compressionType': 'snappy',\n",
       "    'classification': 'parquet',\n",
       "    'typeOfData': 'file'},\n",
       "   'StoredAsSubDirectories': False},\n",
       "  'PartitionKeys': [],\n",
       "  'TableType': 'EXTERNAL_TABLE',\n",
       "  'Parameters': {'compressionType': 'snappy',\n",
       "   'source': 'Google',\n",
       "   'classification': 'parquet',\n",
       "   'projection.enabled': 'false',\n",
       "   'class': 'stock',\n",
       "   'typeOfData': 'file'},\n",
       "  'CreatedBy': 'arn:aws:iam::535146832369:user/tatwan',\n",
       "  'IsRegisteredWithLakeFormation': False,\n",
       "  'CatalogId': '535146832369',\n",
       "  'VersionId': '1',\n",
       "  'IsMultiDialectView': False}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one quick way to view the content is converting into a list\n",
    "tables = wr.catalog.get_tables()\n",
    "list(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fed867",
   "metadata": {},
   "source": [
    "To get specific content you will need to loop through the generator object. This is based on the information above, I can see a dictionary with key value pairs, and nested dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ede3599d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_table my_db EXTERNAL_TABLE\n",
      "test my_db EXTERNAL_TABLE\n",
      "tatwan_stock tatwan_db EXTERNAL_TABLE\n"
     ]
    }
   ],
   "source": [
    "tables = wr.catalog.get_tables()\n",
    "for info in tables:\n",
    "    print(info.get('Name'), info.get('DatabaseName'), info.get('TableType'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f703d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_table {'Columns': [{'Name': 'date', 'Type': 'string', 'Comment': 'Trading Date'}, {'Name': 'open', 'Type': 'double', 'Comment': 'Opening Price'}, {'Name': 'high', 'Type': 'double'}, {'Name': 'low', 'Type': 'double'}, {'Name': 'close', 'Type': 'double', 'Comment': 'Closing Price'}, {'Name': 'volume', 'Type': 'bigint'}], 'Location': 's3://techcatalyst-raw/tatwan/', 'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat', 'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat', 'Compressed': True, 'NumberOfBuckets': -1, 'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe', 'Parameters': {'serialization.format': '1'}}, 'BucketColumns': [], 'SortColumns': [], 'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0', 'compressionType': 'snappy', 'classification': 'parquet', 'typeOfData': 'file'}, 'StoredAsSubDirectories': False}\n",
      "test {'Columns': [{'Name': 'date', 'Type': 'string'}, {'Name': 'open', 'Type': 'double'}, {'Name': 'high', 'Type': 'double'}, {'Name': 'low', 'Type': 'double'}, {'Name': 'close', 'Type': 'double'}, {'Name': 'volume', 'Type': 'bigint'}], 'Location': 's3://techcatalyst-raw/tatwan/raw/', 'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat', 'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat', 'Compressed': True, 'NumberOfBuckets': -1, 'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe', 'Parameters': {'serialization.format': '1'}}, 'BucketColumns': [], 'SortColumns': [], 'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0', 'compressionType': 'snappy', 'classification': 'parquet', 'typeOfData': 'file'}, 'StoredAsSubDirectories': False}\n",
      "tatwan_stock {'Columns': [{'Name': 'date', 'Type': 'string', 'Comment': 'Trading Date'}, {'Name': 'open', 'Type': 'double', 'Comment': 'Opening Price'}, {'Name': 'high', 'Type': 'double'}, {'Name': 'low', 'Type': 'double'}, {'Name': 'close', 'Type': 'double', 'Comment': 'Closing Price'}, {'Name': 'volume', 'Type': 'bigint'}], 'Location': 's3://techcatalyst-raw/tatwan/', 'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat', 'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat', 'Compressed': True, 'NumberOfBuckets': -1, 'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe', 'Parameters': {'serialization.format': '1'}}, 'BucketColumns': [], 'SortColumns': [], 'Parameters': {'CrawlerSchemaDeserializerVersion': '1.0', 'compressionType': 'snappy', 'classification': 'parquet', 'typeOfData': 'file'}, 'StoredAsSubDirectories': False}\n"
     ]
    }
   ],
   "source": [
    "# to get column metadata from the generator you will need to go inside the `StorageDescriptor` key\n",
    "tables = wr.catalog.get_tables()\n",
    "for info in tables:\n",
    "    print(info.get('Name'), info.get('StorageDescriptor'))\n",
    "# you can observer we have a dictionary within a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e519390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date string None\n",
      "open double None\n",
      "high double None\n",
      "low double None\n",
      "close double None\n",
      "volume bigint None\n",
      "date string None\n",
      "open double None\n",
      "high double None\n",
      "low double None\n",
      "close double None\n",
      "volume bigint None\n",
      "date string None\n",
      "open double None\n",
      "high double None\n",
      "low double None\n",
      "close double None\n",
      "volume bigint None\n"
     ]
    }
   ],
   "source": [
    "# to get column metadata from the generator you will need to go inside the `StorageDescriptor` key\n",
    "tables = wr.catalog.get_tables()\n",
    "\n",
    "for info in tables:\n",
    "    for cols in info['StorageDescriptor']['Columns']:\n",
    "        print(cols.get('Name'), cols.get('Type'), cols.get('Columns'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01629e1a",
   "metadata": {},
   "source": [
    "If you want to list all of the buckets available you would use `s3.list_buckets()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba949f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amsu-aws-s3-assessment',\n",
       " 'amsu-s3-cf-website',\n",
       " 'andrea-lm-code-bucket',\n",
       " 'apiproject-build-bucket-su05297',\n",
       " 'austin-lambda-s3',\n",
       " 'aws-athena-query-results-us-east-1-535146832369',\n",
       " 'aws-athena-query-results-us-east-1-535146832369nh',\n",
       " 'aws-athena-query-results-us-east-1-blakeliu',\n",
       " 'aws-athena-query-results-us-east-1-etl-aa',\n",
       " 'aws-athena-query-results-us-east-2-535146832369',\n",
       " 'aws-athena-query-results-us-east-2-535146832369-kb',\n",
       " 'aws-cloudtrail-logs-535146832369-01f575cd',\n",
       " 'aws-cloudtrail-logs-535146832369-9d19bcae',\n",
       " 'aws-logs-535146832369-us-east-1',\n",
       " 'aws-sam-cli-managed-default-samclisourcebucket-1oz2tqbsdli3',\n",
       " 'aws-sam-cli-managed-default-samclisourcebucket-mgdplk7mifi7',\n",
       " 'aws-sam-cli-managed-default-samclisourcebucket-t2x2wb9il3p1',\n",
       " 'aws-sam-cli-managed-default-samclisourcebucket-yh6y67cvomwp',\n",
       " 'axel-di-student14',\n",
       " 'axel-di-student2',\n",
       " 'axel-di-student25',\n",
       " 'axel-di-student8',\n",
       " 'axel-di-team2',\n",
       " 'backend-lambda-java-test',\n",
       " 'barb-lm-code-bucket',\n",
       " 'bored-api-reactjs-build-bucket-rx08052',\n",
       " 'bored-api-rx08052',\n",
       " 'brian-s3-demo',\n",
       " 'bryantbeach-original-images',\n",
       " 'bryantbeach-thumbnail-images',\n",
       " 'bucket-cl05355',\n",
       " 'bucket-terraform-lab4-41091',\n",
       " 'bucket-terraform-lab4-67147',\n",
       " 'bucket-terraform-lab4-82140',\n",
       " 'bucket-terraform-lab4-95407',\n",
       " 'bucket-terraform-lab4-98294',\n",
       " 'buggy-535146832369-us-east-1',\n",
       " 'bugs-app-stack-535146832369-us-east-2',\n",
       " 'calvin-lm-code-bucket',\n",
       " 'capstone-cloud-template-bucket',\n",
       " 'capstone-service-image',\n",
       " 'capstone-techcatalyst-conformed',\n",
       " 'capstone-techcatalyst-raw',\n",
       " 'capstone-techcatalyst-transformed',\n",
       " 'catapiplzwork-cl05355',\n",
       " 'cf-templates-6dazn7owsddb-ap-south-1',\n",
       " 'cf-templates-6dazn7owsddb-us-east-1',\n",
       " 'cf-templates-6dazn7owsddb-us-east-2',\n",
       " 'cf-templates-6dazn7owsddb-us-west-2',\n",
       " 'codepipeline-us-east-2-68716816310',\n",
       " 'connor-awsassignment',\n",
       " 'connor-lambda-s3',\n",
       " 'connor-s3-cf-final-website',\n",
       " 'connor-s3-sqs',\n",
       " 'connorsbucketdemo',\n",
       " 'countingbucket-0',\n",
       " 'countingbucket-1',\n",
       " 'countingbucket-2',\n",
       " 'craig-lm-code-bucket',\n",
       " 'cvent-beanstalk-bucket',\n",
       " 'cvent-recipe-lambda-bucket',\n",
       " 'darrell-assessment-s3',\n",
       " 'david-s3-angular-assesment',\n",
       " 'davidprivatebucket',\n",
       " 'ddd434278923423',\n",
       " 'demo-my-build-bucket-maaike',\n",
       " 'devint-2373',\n",
       " 'devint-4633',\n",
       " 'devint-6326',\n",
       " 'devint-8987',\n",
       " 'devint-auser1',\n",
       " 'devint-auser2',\n",
       " 'devint-dan2238',\n",
       " 'devint-krueger-alt',\n",
       " 'devint-std14-20220329062534398200000001',\n",
       " 'devint-std14-20220329062534458500000001',\n",
       " 'devint-std14-alt',\n",
       " 'devint-std17-20220329063555600900000001',\n",
       " 'devint-std17-20220329063555696700000001',\n",
       " 'devint-std17-alt',\n",
       " 'devint-std18-20220329063643571000000001',\n",
       " 'devint-std18-20220329063644010600000001',\n",
       " 'devint-std30-20220329110905171700000001',\n",
       " 'devint-std4-alt',\n",
       " 'devint-std7-alt',\n",
       " 'devint-std8-20220329064413559800000001',\n",
       " 'devint-std8-20220329064414038300000001',\n",
       " 'devint-student1-2515',\n",
       " 'devint-student1-4241',\n",
       " 'devint-student10-2515',\n",
       " 'devint-student10-4241',\n",
       " 'devint-student11-2515',\n",
       " 'devint-student11-4241',\n",
       " 'devint-student12-2515',\n",
       " 'devint-student12-4241',\n",
       " 'devint-student13-2515',\n",
       " 'devint-student13-4241',\n",
       " 'devint-student14',\n",
       " 'devint-student14-2515',\n",
       " 'devint-student14-4241',\n",
       " 'devint-student15-2515',\n",
       " 'devint-student15-4241',\n",
       " 'devint-student16-2515',\n",
       " 'devint-student16-4241',\n",
       " 'devint-student17-2515',\n",
       " 'devint-student17-4241',\n",
       " 'devint-student18-2515',\n",
       " 'devint-student18-4241',\n",
       " 'devint-student19-2515',\n",
       " 'devint-student19-4241',\n",
       " 'devint-student2-2515',\n",
       " 'devint-student2-4241',\n",
       " 'devint-student20',\n",
       " 'devint-student20-2515',\n",
       " 'devint-student20-4241',\n",
       " 'devint-student21-2515',\n",
       " 'devint-student21-4241',\n",
       " 'devint-student22',\n",
       " 'devint-student22-2515',\n",
       " 'devint-student22-4241',\n",
       " 'devint-student23-2515',\n",
       " 'devint-student23-4241',\n",
       " 'devint-student24-2515',\n",
       " 'devint-student24-4241',\n",
       " 'devint-student25-2515',\n",
       " 'devint-student3-2515',\n",
       " 'devint-student3-4241',\n",
       " 'devint-student4-2515',\n",
       " 'devint-student4-4241',\n",
       " 'devint-student5-2515',\n",
       " 'devint-student5-4241',\n",
       " 'devint-student6-2515',\n",
       " 'devint-student6-4241',\n",
       " 'devint-student7-2515',\n",
       " 'devint-student7-4241',\n",
       " 'devint-student8-2515',\n",
       " 'devint-student8-4241',\n",
       " 'devint-student9',\n",
       " 'devint-student9-2515',\n",
       " 'devint-student9-4241',\n",
       " 'devint-tabrej-alam-alt',\n",
       " 'digitalnomads',\n",
       " 'dooku2',\n",
       " 'dspera-lm-code-bucket',\n",
       " 'dws-di-corleone',\n",
       " 'dws-di-dooku-20220107144313147600000001',\n",
       " 'dws-di-jocker',\n",
       " 'dws-di-krueger',\n",
       " 'dws-di-nosferatu',\n",
       " 'dws-di-nosferatu-alt',\n",
       " 'dws-di-ramkumar',\n",
       " 'dws-di-spike',\n",
       " 'dws-di-student10',\n",
       " 'dws-di-tabrej-alam',\n",
       " 'dws-kattegat-ragnar',\n",
       " 'eamonn-test-bucket-865',\n",
       " 'eksworkshop-535146832369-codepipeline-artifacts',\n",
       " 'elasticbeanstalk-us-east-1-535146832369',\n",
       " 'elasticbeanstalk-us-east-2-535146832369',\n",
       " 'eoanifoeanfeionfiobvnuem',\n",
       " 'ferfybucket-assessment',\n",
       " 'first-bucket-es05260',\n",
       " 'first-web-app-bucket',\n",
       " 'first-web-app-cs08090',\n",
       " 'frontend-codebuildtest',\n",
       " 'georgethomas-original-images',\n",
       " 'georgethomas-thumbnail-images',\n",
       " 'hark-assessment-bucket',\n",
       " 'hk-lambda-bucket',\n",
       " 'jalaunsbucket',\n",
       " 'jalaunsbucket-for-codedeploy',\n",
       " 'jamesbrett-lm-code-bucket',\n",
       " 'joe-lm-code-bucket',\n",
       " 'jokes-build-bucket-su05297',\n",
       " 'jokesbucket-su05297',\n",
       " 'jonathanbardwellsbucket',\n",
       " 'jrrickerson-demo-cohort7',\n",
       " 'juliev1-bucket',\n",
       " 'kanye-build-bucket-mk05255',\n",
       " 'kanye-rest-bucket-mk05255',\n",
       " 'kerim-test-bucket',\n",
       " 'lab-two',\n",
       " 'lcates-original-images',\n",
       " 'lcates-thumbnail-images',\n",
       " 'lincoln-bucket-01',\n",
       " 'maaike-web-bucket',\n",
       " 'magesh-angular-gitlab-app',\n",
       " 'magesh-lambda-s3',\n",
       " 'malcolm-assesment-bucket',\n",
       " 'malcolm-sam-app-ec2-535146832369-us-east-2',\n",
       " 'mchase-s3',\n",
       " 'mini-api-project-auto-deploy-je05425',\n",
       " 'mini-api-project-je',\n",
       " 'mini-project-bucket-es05260',\n",
       " 'mini-react-project-cs08090',\n",
       " 'movie-watcher',\n",
       " 'my-api-build-bucket-et05298',\n",
       " 'my-api-project-bucket-kl08053',\n",
       " 'my-api-project-bucket-se05269',\n",
       " 'my-bucket-ds08057',\n",
       " 'my-bucket-kl08053',\n",
       " 'my-bucket2-et05298',\n",
       " 'my-build-bucket-cl05355',\n",
       " 'my-build-bucket-cs08090',\n",
       " 'my-build-bucket-ds08057',\n",
       " 'my-build-bucket-es05260',\n",
       " 'my-build-bucket-et05298',\n",
       " 'my-build-bucket-jalaun',\n",
       " 'my-build-bucket-je05425',\n",
       " 'my-build-bucket-kl08053',\n",
       " 'my-build-bucket-maaike',\n",
       " 'my-build-bucket-mk05255',\n",
       " 'my-build-bucket-ov05296',\n",
       " 'my-build-bucket-rx08052',\n",
       " 'my-build-bucket-se05269',\n",
       " 'my-build-bucket-sh03945',\n",
       " 'my-build-bucket-su05297',\n",
       " 'my-project-bucket-ov05296',\n",
       " 'my-react-build-api-bucket-se05269',\n",
       " 'my-riddles-build-bucket-ov05296',\n",
       " 'my-s3-bucket-se05269',\n",
       " 'my-test-bucket-student20',\n",
       " 'my-tf-test-bucket-student17',\n",
       " 'my-tf-test-bucket-student20',\n",
       " 'my-tf-test-bucket-student23',\n",
       " 'my-tf-test-bucket-teacher2',\n",
       " 'myawsbucket-ds08057',\n",
       " 'myawsbucket-kl08053',\n",
       " 'myawsbucket-mini-project-ds08057',\n",
       " 'mybucket4efs.com',\n",
       " 'mybucket7707',\n",
       " 'mys3bucket-mk05255',\n",
       " 'natali-assessment-bucket',\n",
       " 'nullwpstage',\n",
       " 'orion-capstone',\n",
       " 'patrickmccardy-lm-code-bucket',\n",
       " 'pokebucket-cs08090',\n",
       " 'radar-app-dev-build-bucket',\n",
       " 'react-jokesapi-ds08057',\n",
       " 'reportbucketnff1',\n",
       " 'repurposed-jalaunsbucket',\n",
       " 'rjp-original-images',\n",
       " 'rjp-thumbnail-images',\n",
       " 'rx08052-web-bucket',\n",
       " 'sagemaker-course-di',\n",
       " 'sagemaker-studio-535146832369-db81fj1x82w',\n",
       " 'sagemaker-us-east-1-535146832369',\n",
       " 'sai-tf-test-bucket-sai',\n",
       " 'se-bucket-100',\n",
       " 'seatownbucket88',\n",
       " 'student-6-alternate',\n",
       " 'student-6-main',\n",
       " 'student17-terraform-provisioners-example',\n",
       " 'student22-ex06',\n",
       " 'su05297-web-bucket',\n",
       " 'super-unique-terraform-bucket',\n",
       " 'teacher2-terraform-provisioners-example',\n",
       " 'techcatalyst-public',\n",
       " 'techcatalyst-raw',\n",
       " 'techcatalyst-transformed',\n",
       " 'terraform-di-student17',\n",
       " 'terraform-di-student17-alternate',\n",
       " 'terraform-lab5--42433',\n",
       " 'terraform-lab5--63647',\n",
       " 'terraform-lab5--64023',\n",
       " 'terraform-lab5--76528',\n",
       " 'terraform-lab5--92518',\n",
       " 'terraform-lab5--95453',\n",
       " 'test-connect-sh03945',\n",
       " 'thisisatestbucket123452439',\n",
       " 'tlopresti-lm-code-bucket',\n",
       " 'troy-aws-assessment-bucket',\n",
       " 'unique-bucket-aws',\n",
       " 'viteexample2',\n",
       " 'vlh-student2',\n",
       " 'vlh-student2-us-east-2',\n",
       " 'will-assessment-bucket',\n",
       " 'will-lambda-s3',\n",
       " 'will-lm-code-bucket',\n",
       " 'wramey-lm-code-bucket']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c742343",
   "metadata": {},
   "source": [
    "To list all objects in a specific bucket you can use the `s3.list_objects`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8815a309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/stage/yellow_tripdata.csv',\n",
       " 's3://techcatalyst-raw/stage/yellow_tripdata.json',\n",
       " 's3://techcatalyst-raw/stage/yellow_tripdata.parquet']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_objects('s3://techcatalyst-raw/stage/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353c997",
   "metadata": {},
   "source": [
    "If you want to download a specific file locally to your machine, you can use the `s3.download`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "37b4d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.download(path='s3://techcatalyst-raw/stocks/GOOG.csv', local_file='./new_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3cd3c6",
   "metadata": {},
   "source": [
    "To upload a file from your local machine to S3 you can use `s3.upload()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7008c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = # ENTER A NAME\n",
    "file_name = #ENTER YOUR FILE NAME YOU WANT TO UPLOAD\n",
    "wr.s3.upload(local_file='new_file.csv',path= f's3://techcatalyst-raw/{your_name}/uploads/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de3597",
   "metadata": {},
   "source": [
    "Let's double check it was uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "06313a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://techcatalyst-raw/tatwan/uploads/test.csv']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.s3.list_objects(f's3://techcatalyst-raw/{your_name}/uploads/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11968c44",
   "metadata": {},
   "source": [
    "## Exercise (using Glue Catalog and Athena)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3797e",
   "metadata": {},
   "source": [
    "You can execute an Athena query directly using the wr.athena.read_sql_query() function. This function runs your SQL query and returns the results as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43555255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read schema from Parquet file.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Configuration ---\n",
    "\n",
    "# create a new Glue Database it should be YOURNAME_TAXI\n",
    "db_name =  # YOUR CODE\n",
    "\n",
    "# Create a table it should be YOURNAME_TRIPDATA\n",
    "table_name =  #YOUR CODE\n",
    "\n",
    "# the path_direcoty to should point to the bucket (main directory)\n",
    "s3_path_directory = #YOUR CODE\n",
    "\n",
    "# The path_file should be the full path to the actual file\n",
    "s3_path_file = # YOUR CODE\n",
    "\n",
    "# --- 2. Get the Schema from the Parquet File Metadata ---\n",
    "\n",
    "# uncomment below if you ran into issues to clean things up and rerun the cell\n",
    "wr.catalog.delete_table_if_exists(database=db_name, table=table_name) \n",
    "\n",
    "# Create the new Glue database first based on the db_name you created\n",
    "# YOUR CODE\n",
    "\n",
    "# This function can extract the schema from our file and returns a tuple: (schema, partitions). We only need the schema. \n",
    "columns_types, partitions_types = wr.s3.read_parquet_metadata(path=s3_path_file)\n",
    "print(\"Successfully read schema from Parquet file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de645b",
   "metadata": {},
   "source": [
    "Inspect and make sure the column types are what you expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a78cd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VendorID': 'int',\n",
       " 'tpep_pickup_datetime': 'timestamp',\n",
       " 'tpep_dropoff_datetime': 'timestamp',\n",
       " 'passenger_count': 'double',\n",
       " 'trip_distance': 'double',\n",
       " 'RatecodeID': 'double',\n",
       " 'store_and_fwd_flag': 'string',\n",
       " 'PULocationID': 'int',\n",
       " 'DOLocationID': 'int',\n",
       " 'payment_type': 'bigint',\n",
       " 'fare_amount': 'double',\n",
       " 'extra': 'double',\n",
       " 'mta_tax': 'double',\n",
       " 'tip_amount': 'double',\n",
       " 'tolls_amount': 'double',\n",
       " 'improvement_surcharge': 'double',\n",
       " 'total_amount': 'double',\n",
       " 'congestion_surcharge': 'double',\n",
       " 'Airport_fee': 'double'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'TATWAN_TRIPDATA' created successfully in database 'TATWAN_TAXI'.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Create the Glue Table with the Explicit Schema ---\n",
    "wr.catalog.create_parquet_table(\n",
    "    database= , # pass the database name\n",
    "    table= , # pass the table name\n",
    "    path= , # use the directoy here \n",
    "    columns_types=  ,  # Pass the schema here\n",
    "    partitions_types= # pass the partition types\n",
    ")\n",
    "print(f\"Table '{table_name}' created successfully in database '{db_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919fe178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Results:\n",
      "   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         1  2024-04-01 00:02:40   2024-04-01 00:30:42              0.0   \n",
      "1         2  2024-04-01 00:56:02   2024-04-01 01:05:09              1.0   \n",
      "2         1  2024-04-01 00:08:32   2024-04-01 00:10:24              1.0   \n",
      "3         2  2024-04-01 00:41:12   2024-04-01 00:55:29              1.0   \n",
      "4         2  2024-04-01 00:48:42   2024-04-01 01:05:30              1.0   \n",
      "\n",
      "   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n",
      "0           5.20         1.0                  N           161             7   \n",
      "1           1.06         1.0                  N           137           164   \n",
      "2           0.70         1.0                  N           236           263   \n",
      "3           5.60         1.0                  N           264           264   \n",
      "4           3.55         1.0                  N           186           236   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             1         29.6    3.5      0.5        8.65           0.0   \n",
      "1             2         10.0    1.0      0.5        0.00           0.0   \n",
      "2             1          5.1    3.5      0.5        2.00           0.0   \n",
      "3             1         25.4    1.0      0.5       10.00           0.0   \n",
      "4             1         20.5    1.0      0.5        5.10           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
      "0                    1.0         43.25                   NaN          NaN  \n",
      "1                    1.0         15.00                   NaN          NaN  \n",
      "2                    1.0         12.10                   NaN          NaN  \n",
      "3                    1.0         37.90                   NaN          NaN  \n",
      "4                    1.0         30.60                   NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "# --- 4. Now you can query it ---\n",
    "query = f\"SELECT * FROM {table_name} LIMIT 5\"\n",
    "\n",
    "# you will use read_sql_query from athena from awswrangler\n",
    "# https://aws-sdk-pandas.readthedocs.io/en/3.12.1/stubs/awswrangler.athena.read_sql_query.html\n",
    "df = # YOUR CODE \n",
    "print(\"\\nQuery Results:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885a459",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Hopefully by now you come to appreciate awswrangler.\n",
    "\n",
    "At its core, it acts as a high-level bridge, extending the power of the popular Pandas library to the AWS cloud environment. Its primary goal is to eliminate the need for extensive \"boilerplate\" code that data scientists and engineers would otherwise have to write using lower-level libraries like Boto3 to interact with services like S3, Redshift, and Athena.\n",
    "\n",
    "__It is an official AWS project and is part of the AWS Professional Services portfolio.__\n",
    "\n",
    "__The Problem It Solves__\n",
    "\n",
    "Imagine you are a data scientist or data engineer working in Python. Your workflow often looks like this:\n",
    "\n",
    "1. __Get Data__: You need to read a large CSV or Parquet file from an Amazon S3 bucket into a Pandas DataFrame.\n",
    "2. __Analyze & Transform__: You perform your analysis, clean the data, and generate new features using Pandas.\n",
    "3. __Store Results__: You need to save your transformed DataFrame back to S3 or load it into a data warehouse like Amazon Redshift for others to use.\n",
    "\n",
    "Without AWS Wrangler, this process involves many manual steps:\n",
    "* Using `Boto3` to connect to S3.\n",
    "* Downloading the file locally or streaming it into memory.\n",
    "* Parsing the file format (e.g., CSV, Parquet) into a DataFrame.\n",
    "* For writing, you'd have to serialize the DataFrame into a specific format (like a CSV string or Parquet file).\n",
    "* Then, use Boto3 again to upload this new file to S3.\n",
    "* All of this requires handling AWS credentials, sessions, and potential errors.\n",
    "\n",
    "__AWS Wrangler simplifies this entire workflow into single-line function calls.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21edd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
