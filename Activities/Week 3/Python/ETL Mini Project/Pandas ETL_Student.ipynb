{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273e3fa7-e061-4000-9999-ed75af7178e2",
   "metadata": {},
   "source": [
    "# Data Engineering Project Instructions\n",
    "\n",
    "## Preparation\n",
    "* You will need to install `boto3`\n",
    "* You will be using `pandas` and `configparser`\n",
    "* You will need to install two additional libraries to support pandas in reading from S3:\n",
    "```bash\n",
    "  conda install s3fs -c conda-forge -y\n",
    "  conda install fsspec -y\n",
    "```\n",
    "\n",
    "## Transformations\n",
    "\n",
    "You will perform the same transformations you did in the previous class, including:\n",
    "\n",
    "* __Calculate Trip Duration in Minutes:__ Call it Trip_Duration.\n",
    "* __Calculate Total Trip Charge:__ Include fare amount, extra, MTA tax, tolls amount, improvement surcharge, congestion surcharge, airport fee, and tip amount. Call it Total_Trip_Charge.\n",
    "* __Add Trip Date Components:__ Add `Trip_Date`, `Trip_Month`, `Trip_Day`, and `Trip_Year`.\n",
    "* __Keep Specific Columns:__ `VendorID`, `passenger_count`, `trip_distance`, `store_and_fwd_flag`, `payment_type`, `Trip_Duration`, `Total_Trip_Charge`, `Trip_Date`, `Trip_Month`, `Trip_Year`, `Trip_Day`\n",
    "* __Reorder the Columns:__ Start with VendorID, followed by all the date/time columns (Date, Year, Month, and Day), then the remaining columns.\n",
    "* __Rename the Columns:__ Rename columns to Vendor_ID, No_of_Passengers, SF_Flag, Payment_Type.\n",
    "\n",
    "## Expectation Part 1\n",
    "\n",
    "* You will create functions to support your ETL Process.\n",
    "* You will need to capture some statistics for each data set you are processing for reference.\n",
    "* You will read from a source S3 bucket (raw) and write into a different S3 bucket (transformed):\n",
    "```python\n",
    "bucket_dest = 'techcatalyst-transformed'\n",
    "bucket_source = 'techcatalyst-raw'\n",
    "```\n",
    "* Make sure that your files \"objects\" are under your name inside the `techcatalyst-transformed` bucket. For example:\n",
    "s3://techcatalyst-transformed/tarek/yellow_tripdata_2024-01_transformed.parquet/Trip_Year=2024/Trip_Month=January/84e2f047dcff4f7183ae25518ecd486b-0.parquet\n",
    "* Create a function that generates an `s3://` URI. The function should take a bucket name, a file name, and then construct an `s3://` URI that points to the object. Refer to this link for more details on S3 URIs. If you forget how the `s3://` URI looks, please log in to AWS and navigate to S3 services to see how an object inside a bucket is being referenced by the `s3://` URI.\n",
    "* Create a **cleanup** function that takes three *parameters*: the DataFrame, the name of the file, and the destination bucket. The function will transform the data, then write out a Parquet file to the destination address.\n",
    "* You may need to create additional functions as necessary.\n",
    "* Finally, once you write the files to S3, you will also write your statistic file. For example., capture number of rows, number of columns, number of columns with null values, date/time of processing. What else can you think of?\n",
    "* While processing each file, it would be great to log something on the screen to show progress or status. Here is an example below:\n",
    "\n",
    "```\n",
    "processing yellow_tripdata_2024-01.parquet\n",
    "...........\n",
    "writing s3://techcatalyst-raw/yellow_tripdata_2024-01.parquet to techcatalyst-transformed bucket\n",
    "...........\n",
    "processing yellow_tripdata_2024-02.parquet\n",
    "...........\n",
    "writing s3://techcatalyst-raw/yellow_tripdata_2024-02.parquet to techcatalyst-transformed bucket\n",
    "...........\n",
    "processing yellow_tripdata_2024-03.parquet\n",
    "...........\n",
    "writing s3://techcatalyst-raw/yellow_tripdata_2024-03.parquet to techcatalyst-transformed bucket\n",
    "...........\n",
    "processing yellow_tripdata_2024-04.parquet\n",
    "...........\n",
    "writing s3://techcatalyst-raw/yellow_tripdata_2024-04.parquet to techcatalyst-transformed bucket\n",
    "...........\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# Expectation Part 2\n",
    "* Once the data has been loaded on AWS, you will need to use Athena to inspect the data to ensure correctness.\n",
    "* Remember, before using Athena, you will need to go through the process of creating the crawlers and other setup steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66fe52dd-1418-4531-8907-152ff454fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376a3bc-4fa1-4c39-a0ac-bc535019a23f",
   "metadata": {},
   "source": [
    "### One way using configparser and a .cfg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cbae517-ae6a-43d2-bbb5-ee350a13fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# read the cfg file\n",
    "config.read('')\n",
    "\n",
    "AWS_ACCESS_KEY = # Add your code \n",
    "AWS_SECRET_KEY = # Add your code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cf18a-d71c-4397-adea-828e174066c2",
   "metadata": {},
   "source": [
    "### Another way to use the dotenv with a `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96daf8-2333-484e-9395-56dffe0a93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY')\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89f9ed3-3b3a-4a76-9c3c-3e0a14505526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish your S3 client \n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-creating-buckets.html\n",
    "# make sure to pass your credentials: aws_access_key_id and aws_secret_access_key\n",
    "s3_client = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2509e16-db47-43f6-9dd6-11aedb91cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the query to get back all the buckets in the account and store in the response variable \n",
    "response = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ea410c-09ed-4e34-ba8a-403956f5556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use either a loop or list comprehension to extract the Bucket Names to a list. This is mainly for your reference in terms what are all the buckets available\n",
    "\n",
    "buckets = [bucket['Name'] for bucket in response['Buckets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d7c951-4790-4474-aeb6-239415746401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We care only about reading from ther 'techcatalyst-raw' bucket\n",
    "\n",
    "bucket_name = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb423ed0-64c6-40c5-9692-e54107419925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ResponseMetadata', 'IsTruncated', 'Contents', 'Name', 'Prefix', 'MaxKeys', 'EncodingType', 'KeyCount'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the objects inside that bucket. Once you get response, you can list all the keys. Remember, like a dictionary you have Key-Value pairs. We care about the Contents key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac155fe-6ab5-4c25-900e-8bd340c3f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_tripdata_2024-01.parquet\n",
      "yellow_tripdata_2024-02.parquet\n",
      "yellow_tripdata_2024-03.parquet\n",
      "yellow_tripdata_2024-04.parquet\n"
     ]
    }
   ],
   "source": [
    "# Print out the objects available. There are the Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90336f7-5d4e-46b0-aaa5-8b0f5d47cd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yellow_tripdata_2024-01.parquet',\n",
       " 'yellow_tripdata_2024-02.parquet',\n",
       " 'yellow_tripdata_2024-03.parquet',\n",
       " 'yellow_tripdata_2024-04.parquet']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store them into a Python list to use later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "774f4959-64c6-4c8e-b3df-fd71f7a3a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e4bfe1-97f7-4dea-9938-15547dddd9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get this error: ImportError: Missing optional dependency 'fsspec'.  Use pip or conda to install fsspec. you just need to uncomment the below\n",
    "# !conda install fsspec -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4daf499-8908-4515-bc49-c5dce4f8a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you did not install s3fs\n",
    "# !conda install s3fs -c conda-forge -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982bfe30-d956-4c1e-b246-cd220ee03378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this example to make sure everything works\n",
    "\n",
    "pd.read_parquet('s3://techcatalyst-raw/yellow_tripdata_2024-01.parquet',\n",
    "               storage_options={\n",
    "                   'key' : AWS_ACCESS_KEY,\n",
    "                   'secret' : AWS_SECRET_KEY\n",
    "               })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5290a-e606-4aca-808f-8a3a1e2a3167",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69cf040a-27dd-4537-925c-3238af0c2bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that generates an s3:// URI \n",
    "# The function takes a bucket name, a file name and then constructs an s3:// uri that points to the object  \n",
    "\n",
    "# https://repost.aws/questions/QUFXlwQxxJQQyg9PMn2b6nTg/what-is-s3-uri-in-simple-storage-service\n",
    "\n",
    "# if you forgot how the s3:// uri looks like then please login to AWS and navigate to S3 services and see how an object inside a bucket is being referenced by the s3:// uri\n",
    "\n",
    "def generate_url(bucket, file):\n",
    "        # your code here \n",
    "        return None # replace None with proper value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaca5291-e03d-4dd9-a8bf-c20493f8d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleanup function that takes three parameters: the DataFrame, the name of the file, the destination bucket\n",
    "\n",
    "# the function will transform the data then write out a Parquet file to the destination address \n",
    "from datetime import datetime\n",
    "\n",
    "def cleanup(df, name, dest):\n",
    "\n",
    "   # Write your code here\n",
    "\n",
    "\n",
    "    return None # replace None with the proper value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c4bb2e8-91a1-4f72-a62f-72fa9ed0a758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing yellow_tripdata_2024-01.parquet\n",
      "...........\n",
      "writing s3://techcatalyst-raw/yellow_tripdata_2024-01.parquet to techcatalyst-transformed bucket\n",
      "...........\n",
      "processing yellow_tripdata_2024-02.parquet\n",
      "...........\n",
      "writing s3://techcatalyst-raw/yellow_tripdata_2024-02.parquet to techcatalyst-transformed bucket\n",
      "...........\n",
      "processing yellow_tripdata_2024-03.parquet\n",
      "...........\n",
      "writing s3://techcatalyst-raw/yellow_tripdata_2024-03.parquet to techcatalyst-transformed bucket\n",
      "...........\n",
      "processing yellow_tripdata_2024-04.parquet\n",
      "...........\n",
      "writing s3://techcatalyst-raw/yellow_tripdata_2024-04.parquet to techcatalyst-transformed bucket\n",
      "...........\n"
     ]
    }
   ],
   "source": [
    "bucket_dest = 'techcatalyst-transformed'\n",
    "bucket_source = 'techcatalyst-raw'\n",
    "\n",
    "# Write your code that leverages the functions you just created \n",
    "# Your functions should perform the ETL but should also print out some status as shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7c93ffd-9eb5-4273-b721-fee1219fbcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>number_of_records</th>\n",
       "      <th>number_of_cols</th>\n",
       "      <th>number_of_cols_with_na</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yellow_tripdata_2024-01.parquet</td>\n",
       "      <td>2964624</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-06-27 14:36:30.323865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yellow_tripdata_2024-02.parquet</td>\n",
       "      <td>3007526</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-06-27 14:55:54.410574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yellow_tripdata_2024-03.parquet</td>\n",
       "      <td>3582628</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-06-27 15:12:01.966005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yellow_tripdata_2024-04.parquet</td>\n",
       "      <td>3514289</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-06-27 15:20:27.438595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file_name number_of_records number_of_cols  \\\n",
       "0  yellow_tripdata_2024-01.parquet           2964624             11   \n",
       "0  yellow_tripdata_2024-02.parquet           3007526             11   \n",
       "0  yellow_tripdata_2024-03.parquet           3582628             11   \n",
       "0  yellow_tripdata_2024-04.parquet           3514289             11   \n",
       "\n",
       "  number_of_cols_with_na                   date_time  \n",
       "0                      3  2024-06-27 14:36:30.323865  \n",
       "0                      3  2024-06-27 14:55:54.410574  \n",
       "0                      3  2024-06-27 15:12:01.966005  \n",
       "0                      3  2024-06-27 15:20:27.438595  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the statistics collected from each file before writing (loading). Can you think of other information you can capture? Maybe stats before and after?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2124fe7-b625-4d79-9f0b-39b1f5959c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
